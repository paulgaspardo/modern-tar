import type { Stats } from "node:fs";
import type { FileHandle } from "node:fs/promises";
import * as fs from "node:fs/promises";
import { cpus } from "node:os";
import * as path from "node:path";
import { Readable } from "node:stream";

import { createTarPacker } from "../tar/packer";
import type { TarHeader } from "../tar/types";
import { normalizeBody } from "../tar/utils";
import { normalizeName } from "./path";
import type { PackOptionsFS, TarSource } from "./types";

type FileBody =
	| { handle: FileHandle; size: number }
	| Uint8Array
	| Readable
	| ReadableStream;

type JobResult = {
	header: TarHeader;
	body?: FileBody;
};

/**
 * @deprecated Use `packTar` instead. This function is now an alias for `packTar`.
 */
export const packTarSources = packTar;

/**
 * Pack a directory or multiple sources into a Node.js `Readable` stream containing
 * tar archive bytes. Can pack either a single directory or an array of sources
 * (files, directories, or raw content).
 *
 * @param sources - Either a directory path string or an array of {@link TarSource} objects.
 * @param options - Optional packing configuration using {@link PackOptionsFS}.
 * @returns Node.js [`Readable`](https://nodejs.org/api/stream.html#class-streamreadable) stream of tar archive bytes
 *
 * @example
 * ```typescript
 * import { packTar } from 'modern-tar/fs';
 * import { createWriteStream } from 'node:fs';
 * import { pipeline } from 'node:stream/promises';
 *
 * // Basic directory packing
 * const tarStream = packTar('/home/user/project');
 * await pipeline(tarStream, createWriteStream('project.tar'));
 *
 * // Pack multiple sources
 * const sources = [
 *   { type: 'file', source: './package.json', target: 'project/package.json' },
 *   { type: 'directory', source: './src', target: 'project/src' },
 *   { type: 'content', content: 'hello world', target: 'project/hello.txt' }
 * ];
 * const archiveStream = packTar(sources);
 * await pipeline(archiveStream, createWriteStream('project.tar'));
 *
 * // With filtering and transformation
 * const filteredStream = packTar('/my/project', {
 *   filter: (path, stats) => !path.includes('node_modules'),
 *   map: (header) => ({ ...header, uname: 'builder' }),
 *   dereference: true  // Follow symlinks
 * });
 * ```
 */
export function packTar(
	sources: TarSource[] | string,
	options: PackOptionsFS = {},
): Readable {
	const stream = new Readable({ read() {} });

	(async () => {
		const packer = createTarPacker(
			(chunk) => stream.push(Buffer.from(chunk)),
			stream.destroy.bind(stream),
			() => stream.push(null),
		);

		const {
			dereference = false,
			filter,
			map,
			baseDir,
			concurrency = cpus().length || 8,
		} = options;

		// Determine input type and resolve directory path if needed
		const isDir = typeof sources === "string";
		const directoryPath = isDir ? path.resolve(sources) : null;

		// Create initial job queue from directory contents or provided sources
		const jobs: TarSource[] = isDir
			? // biome-ignore lint/style/noNonNullAssertion: isDir matches this check.
				(await fs.readdir(directoryPath!, { withFileTypes: true })).map(
					(entry) => ({
						type: entry.isDirectory() ? "directory" : "file",
						// biome-ignore lint/style/noNonNullAssertion: Checked above.
						source: path.join(directoryPath!, entry.name),
						target: entry.name,
					}),
				)
			: (sources as TarSource[]);

		const results = new Map<number, JobResult | null>();
		// Resolvers is used to notify the writer when a job result is ready.
		const resolvers = new Map<number, () => void>();
		// inodes can be 64-bit, so use bigint for correctness.
		const seenInodes = new Map<bigint, string>();

		let jobIndex = 0;
		let writeIndex = 0;
		let activeWorkers = 0;
		let allJobsQueued = false;

		const writer = async () => {
			// Pre-allocate read buffers, but only lazily allocate the large one if needed.
			const readBufferSmall = Buffer.alloc(64 * 1024); // 64KB
			let readBufferLarge: Buffer | null = null; // 512KB

			while (true) {
				if (stream.destroyed) return;

				// Terminate only when all jobs generated by workers have been written.
				if (allJobsQueued && writeIndex >= jobs.length) break;

				// Wait for the next result if it's not ready yet.
				if (!results.has(writeIndex)) {
					await new Promise<void>((resolve) =>
						resolvers.set(writeIndex, resolve),
					);
					continue;
				}

				// Write out all ready results in order. Clean up maps to free memory.
				// biome-ignore lint/style/noNonNullAssertion: .has check above.
				const result = results.get(writeIndex)!;
				results.delete(writeIndex);
				resolvers.delete(writeIndex);

				// Skip null results (filtered out).
				if (!result) {
					writeIndex++;
					continue;
				}

				packer.add(result.header);

				// Write file content if present.
				if (result.body) {
					if (result.body instanceof Uint8Array) {
						if (result.body.length > 0) packer.write(result.body);
					} else if (
						result.body instanceof Readable ||
						result.body instanceof ReadableStream
					) {
						try {
							for await (const chunk of result.body) {
								if (stream.destroyed) break;
								packer.write(
									chunk instanceof Uint8Array ? chunk : Buffer.from(chunk),
								);
							}
						} catch (error) {
							stream.destroy(error as Error);
							return;
						}
					} else {
						const { handle, size } = result.body;

						// Select a 64KB or 512KB buffer based on file size > 1MB.
						const readBuffer =
							size > 1048576
								? // biome-ignore lint/suspicious/noAssignInExpressions: Cleaner.
									(readBufferLarge ??= Buffer.alloc(512 * 1024))
								: readBufferSmall;

						try {
							let bytesLeft = size;
							while (bytesLeft > 0 && !stream.destroyed) {
								const toRead = Math.min(bytesLeft, readBuffer.length);

								const { bytesRead } = await handle.read(
									readBuffer,
									0,
									toRead,
									null,
								);

								if (bytesRead === 0) break; // EOF

								packer.write(readBuffer.subarray(0, bytesRead));
								bytesLeft -= bytesRead;
							}
						} catch (error) {
							stream.destroy(error as Error);
							return;
						} finally {
							await handle.close();
						}
					}
				}
				packer.endEntry();
				writeIndex++;
			}
		};

		const controller = () => {
			if (stream.destroyed || allJobsQueued) return;

			// Start new workers while under concurrency limit and jobs remain.
			while (activeWorkers < concurrency && jobIndex < jobs.length) {
				activeWorkers++;
				const currentIndex = jobIndex++;

				processJob(jobs[currentIndex], currentIndex)
					.catch(stream.destroy.bind(stream))
					.finally(() => {
						activeWorkers--;
						controller(); // Check for more work.
					});
			}

			// If no active workers and all jobs have been queued, signal completion.
			if (activeWorkers === 0 && jobIndex >= jobs.length) {
				allJobsQueued = true;
				resolvers.get(writeIndex)?.(); // Unblock writer if it's waiting.
			}
		};

		const processJob = async (job: TarSource, index: number) => {
			let jobResult: JobResult | null = null;

			// Normalize target path to use forward slashes.
			const target = normalizeName(job.target);

			try {
				if (job.type === "content" || job.type === "stream") {
					let body: FileBody;
					let size: number;
					const isDir = target.endsWith("/");

					if (job.type === "stream") {
						if (
							typeof job.size !== "number" ||
							(!isDir && job.size <= 0) ||
							(isDir && job.size !== 0)
						)
							throw new Error(
								isDir
									? "Streams for directories must have size 0."
									: "Streams require a positive size.",
							);

						size = job.size;
						body = job.content;
					} else {
						const content = await normalizeBody(job.content);
						size = content.length;
						body = content;
					}

					const stat = {
						size: isDir ? 0 : size,
						isFile: () => !isDir,
						isDirectory: () => isDir,
						isSymbolicLink: () => false,
						mode: job.mode,
						mtime: job.mtime ?? new Date(),
						uid: job.uid ?? 0,
						gid: job.gid ?? 0,
					} as Stats;

					if (filter && !filter(target, stat)) return;

					let header: TarHeader = {
						name: target,
						type: isDir ? "directory" : "file",
						size: isDir ? 0 : size,
						mode: stat.mode,
						mtime: stat.mtime,
						uid: stat.uid,
						gid: stat.gid,
						uname: job.uname,
						gname: job.gname,
					};

					if (map) header = map(header);

					jobResult = {
						header,
						body: isDir ? undefined : body,
					};

					return;
				}

				let stat = await fs.lstat(job.source, { bigint: true });

				// Optionally follow symlinks to their targets.
				if (dereference && stat.isSymbolicLink()) {
					const linkTarget = await fs.readlink(job.source);
					const resolved = path.resolve(path.dirname(job.source), linkTarget);

					// Ensure symlinks do not point outside the base directory.
					const resolvedBase = baseDir ?? directoryPath ?? process.cwd();
					if (
						!resolved.startsWith(resolvedBase + path.sep) &&
						resolved !== resolvedBase
					) {
						return; // Skip and do no further work.
					}

					stat = await fs.stat(job.source, { bigint: true }); // Follow the link.
				}

				// BigIntStats have the same methods as regular Stats.
				if (filter && !filter(target, stat as unknown as Stats)) return;

				// Cast bigint fields to number where safe.
				let header: TarHeader = {
					name: target,
					size: 0,
					mode: job.mode ?? Number(stat.mode),
					mtime: job.mtime ?? stat.mtime,
					uid: job.uid ?? Number(stat.uid),
					gid: job.gid ?? Number(stat.gid),
					uname: job.uname,
					gname: job.gname,
					type: "file", // Default type
				};

				let body: FileBody | undefined;
				if (stat.isDirectory()) {
					header.type = "directory";
					header.name = target.endsWith("/") ? target : `${target}/`;

					// Enqueue children for processing.
					try {
						for (const d of await fs.readdir(job.source, {
							withFileTypes: true,
						})) {
							jobs.push({
								type: d.isDirectory() ? "directory" : "file",
								source: path.join(job.source, d.name),
								target: `${header.name}${d.name}`, // Reuse normalized parent path.
							});
						}
					} catch {}
				} else if (stat.isSymbolicLink()) {
					// Store the link itself, not the target file.
					header.type = "symlink";
					header.linkname = await fs.readlink(job.source);
				} else if (stat.isFile()) {
					header.size = Number(stat.size);

					// Deduplicate hard links with inode number.
					if (stat.nlink > 1 && seenInodes.has(stat.ino)) {
						header.type = "link";
						// biome-ignore lint/style/noNonNullAssertion: .has check above.
						header.linkname = seenInodes.get(stat.ino)!;
						header.size = 0;
					} else {
						// Else handle as a regular file.
						if (stat.nlink > 1) seenInodes.set(stat.ino, target);
						if (header.size > 0) {
							// If the file is small (< 32KB), read it into a buffer immediately.
							if (header.size < 32 * 1024) {
								body = await fs.readFile(job.source);
							} else {
								// For large files, stream from from disk when needed.
								body = {
									handle: await fs.open(job.source, "r"),
									size: header.size,
								};
							}
						}
					}
				} else {
					return; // Skip unsupported file types (sockets, FIFOs, etc.)
				}

				if (map) header = map(header);
				jobResult = { header, body };
			} finally {
				// Store the result (or null if filtered out) and notify the writer.
				results.set(index, jobResult);
				resolvers.get(index)?.();
			}
		};

		// Start the controller and writer.
		controller();
		await writer();

		// Finalize the packer to write end-of-archive blocks.
		if (!stream.destroyed) packer.finalize();
	})().catch((error) => stream.destroy(error));

	return stream;
}
